## 모델 평가 지표란?
우리는 모델을 만들어도, 해당 모델이 얼마나 좋은 성능을 내는지 올바르게 평가해야 한다.
그럴 때 여러 측면에서 효율적으로 평가하기 위해 여러 종류의 평가 지표를 사용한다.

### 대표적인 평가 지표
#### 분류 평가 지표
1. Precision (정확도/정밀도)
	1. 양성 예측 중 실제 양성의 비율로, 스팸 필터에서는 정상 메일을 스팸 필터로 오분류하지 않도록 해야 함
2. Recall (재현율)
	1. 실제 양성 중 양성으로 예측한 비율로, 암 진단에서는 높은 재현율이 중요하여 실제 환자를 놓치지 않도록 해야 함
3. F1-Score
	1. Precision과 Recall의 조화평균으로, 둘 다 높을 때만 높은 값을 가짐
4. Macro-average
	1. 각 클래스의 지표를 평균하여 모든 클래스를 동등하게 취급하고, 가중 평균은 클래스 빈도로 가중하여 반영함
5. Macro F1
	1. 각 클래스의 F1-Score를 단순 평균한 것으로, 데이터가 적은 클래스도 동등하게 취급하여 전체적인 모델 성능 평가함
6. Weighted F1
	1. 각 클래스의 샘플 수에 비례하여 가중치를 준 평균으로, 실제 서비스에서 자주 나타나는 클래스의 성능 평가를 중시함
7. ROC-AUC(Area Under Curve)
	1. 모든 임계값에서의 성능을 종합한 지표로, 0.5는 무작위 예측, 1.0은 완벽한 분류를 의미함
#### 탐지 평가 지표
1. IoU(Intersection over Union)
	1. 예측 박스와 정답 박스가 겹치는 영역을 전체 영역으로 나눈 값임 = 정답 박스에 얼마나 근접하게 예측했는가
	2. 자율 주행에서는 보행자 탐지의 경우 안전을 위해 IoU 임계값을 0.3으로 낮춰 탐지, 차선 인식은 정확도를 위해 0.7로 높여 평가함
2. AP(Average Precision)
	1. 재현율을 0%부터 100%까지 변화시키면서 각 지점의 정밀도를 평균한 값임
	2. 한 클래스에 대한 종합적인 탐지 성능을 나타내며, 80% 이상이면 실용적인 수준임
3. mAP@0.5
	1. mAP@0.5는 IoU 0.5 기준으로 모든 클래스의 AP를 평균한 값임
	2. COCO 데이터셋의 mAP@[0.5:0.95]는 IoU를 0.5부터 0.95까지 0.05 간격으로 변화시키며 계산한 평균임
	3. 최신 모델들은 COCO mAP 50% 이상 달성

> 나는 일반적으로 mAP@0.5, mAP@0.5:0.95, Precision, Recall의 지표를 살펴보고 box_loss나 cls_loss 정도를 시각화하여 분석한다.